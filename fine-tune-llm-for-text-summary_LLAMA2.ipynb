{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    "# **FINE TUNING LLM FOR TEXT SUMMARIZATION**\n",
    "### DATASET - [CNN Dailymail Dataset](https://huggingface.co/datasets/cnn_dailymail).\n",
    "- The **CNN / DailyMail Dataset** is an English-language dataset containing just over 300k unique news articles as written by journalists at CNN and the Daily Mail.\n",
    "#### Data Fields\n",
    "- **id**: a string containing the heximal formated SHA1 hash of the url where the story was retrieved from\n",
    "- **article**: a string containing the body of the news article\n",
    "- **highlights**: a string containing the highlight of the article as written by the article author\n",
    "<br>\n",
    "\n",
    "### MODEL - [Llama2](https://huggingface.co/NousResearch/Llama-2-7b-hf).\n",
    "- **Llama 2** is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. \n",
    "- **Model Architecture**: Llama 2 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align to human preferences for helpfulness and safety.\n",
    "---\n",
    "### ***Sai Vishwas Aluvala ***\n",
    "### ***FINAL YEAR - IIT Kharagpur***\n",
    "### ***saivishwas2003@gmail.com***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-27T16:31:51.695556Z",
     "iopub.status.busy": "2023-12-27T16:31:51.695170Z",
     "iopub.status.idle": "2023-12-27T16:33:18.569941Z",
     "shell.execute_reply": "2023-12-27T16:33:18.568680Z",
     "shell.execute_reply.started": "2023-12-27T16:31:51.695524Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.31\n",
      "  Obtaining dependency information for transformers==4.31 from https://files.pythonhosted.org/packages/21/02/ae8e595f45b6c8edee07913892b3b41f5f5f273962ad98851dc6a564bbb9/transformers-4.31.0-py3-none-any.whl.metadata\n",
      "  Downloading transformers-4.31.0-py3-none-any.whl.metadata (116 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.9/116.9 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.31) (3.12.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31) (0.19.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31) (2023.8.8)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.31) (2.31.0)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.31)\n",
      "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31) (0.4.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31) (4.66.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.31) (2023.12.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.31) (4.5.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers==4.31) (3.0.9)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.31) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.31) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.31) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.31) (2023.11.17)\n",
      "Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m71.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tokenizers, transformers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.15.0\n",
      "    Uninstalling tokenizers-0.15.0:\n",
      "      Successfully uninstalled tokenizers-0.15.0\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.36.0\n",
      "    Uninstalling transformers-4.36.0:\n",
      "      Successfully uninstalled transformers-4.36.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "kaggle-environments 1.14.3 requires transformers>=4.33.1, but you have transformers 4.31.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed tokenizers-0.13.3 transformers-4.31.0\n",
      "Collecting peft\n",
      "  Obtaining dependency information for peft from https://files.pythonhosted.org/packages/8b/1b/aee2a330d050c493642d59ba6af51f3910cb138ea48ede228c84c204a5af/peft-0.7.1-py3-none-any.whl.metadata\n",
      "  Downloading peft-0.7.1-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft) (21.3)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.0.0)\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft) (4.31.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft) (4.66.1)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.25.0)\n",
      "Requirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft) (0.4.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.19.4)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (3.12.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2023.12.2)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (4.5.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft) (3.0.9)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2023.8.8)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.13.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2023.11.17)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n",
      "Downloading peft-0.7.1-py3-none-any.whl (168 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.3/168.3 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hInstalling collected packages: peft\n",
      "Successfully installed peft-0.7.1\n"
     ]
    }
   ],
   "source": [
    "!pip install -q -U bitsandbytes\n",
    "!pip install transformers==4.31\n",
    "# !pip install -q -U git+https://github.com/huggingface/peft.git\n",
    "!pip install peft\n",
    "!pip install -q datasets\n",
    "!pip install -qqq trl==0.7.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-27T16:34:37.978819Z",
     "iopub.status.busy": "2023-12-27T16:34:37.978097Z",
     "iopub.status.idle": "2023-12-27T16:34:37.985202Z",
     "shell.execute_reply": "2023-12-27T16:34:37.984116Z",
     "shell.execute_reply.started": "2023-12-27T16:34:37.978782Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datasets import Dataset, load_dataset\n",
    "from datasets import load_dataset, load_metric\n",
    "from transformers import pipeline, set_seed\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **THE CNN DAILY MAIL DATASET**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-27T16:34:41.225550Z",
     "iopub.status.busy": "2023-12-27T16:34:41.224562Z",
     "iopub.status.idle": "2023-12-27T16:38:00.499042Z",
     "shell.execute_reply": "2023-12-27T16:38:00.497647Z",
     "shell.execute_reply.started": "2023-12-27T16:34:41.225512Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc97b15d3fae42b6a3ba99d8489f6d82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/3.51k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74e93ee7689c4f5a856eed9e3a9a2b40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/1.61k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset cnn_dailymail/3.0.0 (download: 558.32 MiB, generated: 1.28 GiB, post-processed: Unknown size, total: 1.82 GiB) to /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/3cb851bf7cf5826e45d49db2863f627cba583cbc32342df7349dfe6c38060234...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72e83d043c674a929a7b1fc035a46912",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2800adb2e0aa4507a581461331607cd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/159M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b36bb96c20c9499e8eeb4518792c24cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/376M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52222af363604342b894ed831c9c3a34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/572k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d5ed6c108db4be689e6fd2ce9fa62a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/12.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "258acae99bf942d5849f9f73ca6ca592",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/661k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "705e93a0fe6a484b929e9c1fe1459934",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/287113 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/13368 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/11490 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset cnn_dailymail downloaded and prepared to /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/3cb851bf7cf5826e45d49db2863f627cba583cbc32342df7349dfe6c38060234. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "728d19bc96de43cc8b7631f0afeba0f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['article', 'highlights', 'id'],\n",
       "        num_rows: 287113\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['article', 'highlights', 'id'],\n",
       "        num_rows: 13368\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['article', 'highlights', 'id'],\n",
       "        num_rows: 11490\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "huggingface_dataset_name = \"cnn_dailymail\"\n",
    "\n",
    "dataset = load_dataset(huggingface_dataset_name, \"3.0.0\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOOKING OUT FOR A SAMPLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-27T16:38:00.501820Z",
     "iopub.status.busy": "2023-12-27T16:38:00.501168Z",
     "iopub.status.idle": "2023-12-27T16:38:00.510261Z",
     "shell.execute_reply": "2023-12-27T16:38:00.509295Z",
     "shell.execute_reply.started": "2023-12-27T16:38:00.501782Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article (excerpt of 500 characters, total length: 3192):\n",
      "(CNN) -- Usain Bolt rounded off the world championships Sunday by claiming his third gold in Moscow as he anchored Jamaica to victory in the men's 4x100m relay. The fastest man in the world charged clear of United States rival Justin Gatlin as the Jamaican quartet of Nesta Carter, Kemar Bailey-Cole, Nickel Ashmeade and Bolt won in 37.36 seconds. The U.S finished second in 37.56 seconds with Canada taking the bronze after Britain were disqualified for a faulty handover. The 26-year-old Bolt has n\n",
      "\n",
      "Summary (length: 180):\n",
      "Usain Bolt wins third gold of world championship .\n",
      "Anchors Jamaica to 4x100m relay victory .\n",
      "Eighth gold at the championships for Bolt .\n",
      "Jamaica double up in women's 4x100m relay .\n"
     ]
    }
   ],
   "source": [
    "sample = dataset[\"train\"][1]\n",
    "print(f\"\"\"Article (excerpt of 500 characters, total length: {len(sample[\"article\"])}):\"\"\")\n",
    "print(sample[\"article\"][:500])\n",
    "print(f'\\nSummary (length: {len(sample[\"highlights\"])}):')\n",
    "print(sample[\"highlights\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PRE PROCESSING FUNCTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Converting the `article text` and `summary` to a `prompt`:**\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "    Summarize the following conversation.\n",
    "    \n",
    "    ### Input:\n",
    "    (CNN) -- Usain Bolt rounded off the world championships Sunday by claiming his third \n",
    "    gold in Moscow as he anchored Jamaica to\n",
    "    victory in the men's 4x100m relay. The fastest man in the world charged clear of United \n",
    "    States rival Justin Gatlin as the Jamaican\n",
    "    quartet of Nesta Carter, Kemar Bailey-Cole, Nickel Ashmeade and Bolt won in 37.36 \n",
    "    seconds. The U.S finished second in 37.56 seconds \n",
    "    with Canada taking the bronze after Britain were disqualified for a faulty handover. \n",
    "    The 26-year-old Bolt has n......\n",
    "    \n",
    "\n",
    "    ### Summary:\n",
    "\n",
    "    Usain Bolt wins third gold of world championship .\n",
    "    Anchors Jamaica to 4x100m relay victory .\n",
    "    Eighth gold at the championships for Bolt .\n",
    "    Jamaica double up in women's 4x100m relay .\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-27T16:38:00.511767Z",
     "iopub.status.busy": "2023-12-27T16:38:00.511450Z",
     "iopub.status.idle": "2023-12-27T16:38:00.790081Z",
     "shell.execute_reply": "2023-12-27T16:38:00.789117Z",
     "shell.execute_reply.started": "2023-12-27T16:38:00.511741Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def format_instruction(dialogue: str, summary: str):\n",
    "    return f\"\"\"### Instruction:\n",
    "Summarize the following conversation.\n",
    "\n",
    "### Input:\n",
    "{dialogue.strip()}\n",
    "\n",
    "### Summary:\n",
    "{summary}\n",
    "\"\"\".strip()\n",
    "\n",
    "def generate_instruction_dataset(data_point):\n",
    "\n",
    "    return {\n",
    "        \"article\": data_point[\"article\"],\n",
    "        \"highlights\": data_point[\"highlights\"],\n",
    "        \"text\": format_instruction(data_point[\"article\"],data_point[\"highlights\"])\n",
    "    }\n",
    "\n",
    "def process_dataset(data: Dataset):\n",
    "    return (\n",
    "        data.shuffle(seed=42)\n",
    "        .map(generate_instruction_dataset).remove_columns(['id'])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-27T16:38:00.793330Z",
     "iopub.status.busy": "2023-12-27T16:38:00.792913Z",
     "iopub.status.idle": "2023-12-27T16:38:52.679132Z",
     "shell.execute_reply": "2023-12-27T16:38:52.678100Z",
     "shell.execute_reply.started": "2023-12-27T16:38:00.793294Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a03965cccab4c59853913a5c610fe77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/287113 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02b03450b7fc4341b40891fd1c0c57f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13368 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09eb586fe1184f33a246a450cc489cdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13368 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['article', 'highlights', 'text'],\n",
       "     num_rows: 1000\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['article', 'highlights', 'text'],\n",
       "     num_rows: 100\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['article', 'highlights', 'text'],\n",
       "     num_rows: 100\n",
       " }))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## APPLYING PREPROCESSING ON WHOLE DATASET\n",
    "dataset[\"train\"] = process_dataset(dataset[\"train\"])\n",
    "dataset[\"test\"] = process_dataset(dataset[\"validation\"])\n",
    "dataset[\"validation\"] = process_dataset(dataset[\"validation\"])\n",
    "\n",
    "# Select 1000 rows from the training split\n",
    "train_data = dataset['train'].shuffle(seed=42).select([i for i in range(1000)])\n",
    "\n",
    "# Select 100 rows from the test and validation splits\n",
    "test_data = dataset['test'].shuffle(seed=42).select([i for i in range(100)])\n",
    "validation_data = dataset['validation'].shuffle(seed=42).select([i for i in range(100)])\n",
    "\n",
    "train_data,test_data,validation_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **THE LLAMA-2 7B MODEL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-27T16:38:52.680652Z",
     "iopub.status.busy": "2023-12-27T16:38:52.680311Z",
     "iopub.status.idle": "2023-12-27T16:41:35.687771Z",
     "shell.execute_reply": "2023-12-27T16:41:35.686649Z",
     "shell.execute_reply.started": "2023-12-27T16:38:52.680620Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15b73e21c28040a9bf1ddfbc1c24f4b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/583 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1422fc14984147b4b5d2a93022f33433",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1fbdd8329ef4eef99271f74b6e6d858",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39a18673f9ea472697a1aa0b03d848f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87e4da53f0254199aee525ca7024fda2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edb375e583294a11a4552ecfbd2c0cd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afafa5ec6b5f4bb8a4de6d819983d438",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/179 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9eb7850e15a24d9b84d4bd0e7d0284d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/746 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de4a45727a46441fb4d2303b81875e1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "212344017a5f4ea19e50b8f9cb0f38c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0178285c17e43808865315582545a8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/435 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "model_id =  \"NousResearch/Llama-2-7b-hf\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map=\"auto\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ZERO-SHOT INFERENCE WITH LLAMA-2 7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-27T16:41:35.689566Z",
     "iopub.status.busy": "2023-12-27T16:41:35.689229Z",
     "iopub.status.idle": "2023-12-27T16:41:54.453284Z",
     "shell.execute_reply": "2023-12-27T16:41:54.452052Z",
     "shell.execute_reply.started": "2023-12-27T16:41:35.689536Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "\n",
      "Summarize the following conversation.\n",
      "\n",
      "### Input:\n",
      "A 22-year-old man bought a coin that he believes could date back more than 2,000 years for just £29. Coin collector Stephen Creswell snapped up what he thinks is a half shekel from Tyre - which dates back to 126BC - at an auction in Wolverhampton, Staffordshire. The coins are believed by Biblical scholars to have been the same as those used to pay Judas Iscariot his 30 pieces of silver for betraying Jesus. Lost treasure? The coin found by Stephen Creswell, 22, which he believes to be half a shekel from Tyre . Collector: Mr Creswell, pictured, bought the coin for just £29 at an auction in Wolverhampton, Staffordshire . But one expert claims the coin is unlikely to be an original and believes Mr Cresswell might have a 'mock up'. The battered coin weighs 7.7grams, half of a full shekel's weight. The earliest reference of the coin comes in 126BC and it is understood that they remained in circulation. Shekels were used for temple tax in ancient Jerusalem due to their relatively higher concentration of silver than other Roman coins. In Matthew 27:3 it is said: 'Then when Judas, who had betrayed Him, saw that He had been condemned, he felt remorse and returned the thirty pieces of silver to the chief priests and elders...' In some translations of the bible, the word 'silver' is replaced with 'shekels'. Mr Creswell, from Dudley, West Midlands said that he bought the coin as part of a selection but quickly realised he had 'something special'. He said: 'As soon as I saw how old it was I knew I had something special on my hands. I had no idea what it was though until I typed it in to the search engine. 'I'm not really a coin collector, but I just love to grab a bargain.' But after looking at images of the coin, one expert said he does not believe it to be a shekel as Mr Cresswell had hoped. Payment: The coins are believed by Biblical scholars to have been the same as those used to pay Judas Iscariot his 30 pieces of silver for betraying Jesus, illustrated in this image above . Professional numismatist Mike Vosper told MailOnline: 'I can confirm that this is not even a coin but either a contemporary forgery, although this is doubtful, or more like a modern mock up of a \"coin\" believed to be some kind of shekel.' Mr Vosper, who runs Mike R Vosper Coins with his partner Viv, said it was the fabric of the coin and the fact that it has half its surface missing that gives it away. Viv added: 'Some coins have surface deposits but nothing like that'. Mr Vosper added that coins such as that is not likely to be found in the UK. The coin, if real, could fetch up to £500 if it was to be sold. Mr Creswell has no intention of selling it on as it reminds me of playing with artifacts when he was younger. He said: 'We had these old coins lying around when I was a kid. They always fascinated me because of the places they have travelled and the hands they could have passed through.' Mr Cresswell started attending auctions as a teenager and said he loved their competitive nature. He said: 'Bidding against other people is part way between placing a bet and taking part in a competition. 'I hate leaving any auction empty handed. My home is full of quirky objects that I've picked up along the way. Without doubt, the half shekel is my favourite so far.'\n",
      "\n",
      "### Summary:\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "Collector Stephen Creswell, 22, bought the coin at auction for just £29 .\n",
      "He believes it is half a shekel from Tyre which dates back to 126BC .\n",
      "Coins were used to pay tax in ancient Jerusalem and are cited in the bible .\n",
      "But coin expert said its appearance means it is unlikely to be an original .\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ZERO SHOT:\n",
      "\n",
      "Summarize the following conversation.\n",
      "\n",
      "### Input:\n",
      "A 22-year-old man bought a coin that he believes could date back more than 2,000 years for just £29. Coin collector Stephen Creswell snapped up what he thinks is a half shekel from Tyre - which dates back to 126BC - at an auction in Wolverhampton, Staffordshire. The coins are believed by Biblical scholars to have been the same as those used to pay Judas Iscariot his 30 pieces of silver for betraying Jesus. Lost treasure? The coin found by Stephen Creswell, 22, which he believes to be half a shekel from Tyre . Collector: Mr Creswell, pictured, bought the coin for just £29 at an auction in Wolverhampton, Staffordshire . But one expert claims the coin is unlikely to be an original and believes Mr Cresswell might have a 'mock up'. The battered coin weighs 7.7grams, half of a full shekel's weight. The earliest reference of the coin comes in 126BC and it is understood that they remained in circulation. Shekels were used for temple tax in ancient Jerusalem due to their relatively higher concentration of silver than other Roman coins. In Matthew 27:3 it is said: 'Then when Judas, who had betrayed Him, saw that He had been condemned, he felt remorse and returned the thirty pieces of silver to the chief priests and elders...' In some translations of the bible, the word 'silver' is replaced with 'shekels'. Mr Creswell, from Dudley, West Midlands said that he bought the coin as part of a selection but quickly realised he had 'something special'. He said: 'As soon as I saw how old it was I knew I had something special on my hands. I had no idea what it was though until I typed it in to the search engine. 'I'm not really a coin collector, but I just love to grab a bargain.' But after looking at images of the coin, one expert said he does not believe it to be a shekel as Mr Cresswell had hoped. Payment: The coins are believed by Biblical scholars to have been the same as those used to pay Judas Iscariot his 30 pieces of silver for betraying Jesus, illustrated in this image above . Professional numismatist Mike Vosper told MailOnline: 'I can confirm that this is not even a coin but either a contemporary forgery, although this is doubtful, or more like a modern mock up of a \"coin\" believed to be some kind of shekel.' Mr Vosper, who runs Mike R Vosper Coins with his partner Viv, said it was the fabric of the coin and the fact that it has half its surface missing that gives it away. Viv added: 'Some coins have surface deposits but nothing like that'. Mr Vosper added that coins such as that is not likely to be found in the UK. The coin, if real, could fetch up to £500 if it was to be sold. Mr Creswell has no intention of selling it on as it reminds me of playing with artifacts when he was younger. He said: 'We had these old coins lying around when I was a kid. They always fascinated me because of the places they have travelled and the hands they could have passed through.' Mr Cresswell started attending auctions as a teenager and said he loved their competitive nature. He said: 'Bidding against other people is part way between placing a bet and taking part in a competition. 'I hate leaving any auction empty handed. My home is full of quirky objects that I've picked up along the way. Without doubt, the half shekel is my favourite so far.'\n",
      "\n",
      "### Summary:\n",
      "\n",
      "The man bought a coin that he believes could date back more than 2,000 years for just £29. Coin collector Stephen Creswell snapped up what he thinks is a half shekel from Tyre - which dates back to 126BC - at an auction in Wolverhampton, Staffordshire. The coins are believed by Biblical scholars to have been the same as those used to pay Judas Is\n"
     ]
    }
   ],
   "source": [
    "index = 2\n",
    "\n",
    "dialogue = test_data['article'][index]\n",
    "summary = test_data['highlights'][index]\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "### Input:\n",
    "{dialogue}\n",
    "\n",
    "### Summary:\n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors='pt')\n",
    "output = tokenizer.decode(\n",
    "    model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_new_tokens=100,\n",
    "    )[0],\n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "dash_line = '-'.join('' for x in range(100))\n",
    "print(dash_line)\n",
    "print(f'INPUT PROMPT:\\n{prompt}')\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n",
    "print(dash_line)\n",
    "print(f'MODEL GENERATION - ZERO SHOT:\\n{output}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **TRAINING STEP (FINE TUNING)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-27T16:41:54.454905Z",
     "iopub.status.busy": "2023-12-27T16:41:54.454570Z",
     "iopub.status.idle": "2023-12-27T16:41:54.541115Z",
     "shell.execute_reply": "2023-12-27T16:41:54.540034Z",
     "shell.execute_reply.started": "2023-12-27T16:41:54.454878Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-27T16:41:54.543076Z",
     "iopub.status.busy": "2023-12-27T16:41:54.542626Z",
     "iopub.status.idle": "2023-12-27T16:41:54.978099Z",
     "shell.execute_reply": "2023-12-27T16:41:54.976934Z",
     "shell.execute_reply.started": "2023-12-27T16:41:54.543034Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 16777216 || all params: 3517190144 || trainable%: 0.477006226934315\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=64,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"], #specific to Llama models.\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-27T16:41:54.980140Z",
     "iopub.status.busy": "2023-12-27T16:41:54.979762Z",
     "iopub.status.idle": "2023-12-27T16:41:54.989521Z",
     "shell.execute_reply": "2023-12-27T16:41:54.988301Z",
     "shell.execute_reply.started": "2023-12-27T16:41:54.980109Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "OUTPUT_DIR = \"llama2-docsum-adapter\"\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    logging_steps=1,\n",
    "    learning_rate=1e-4,\n",
    "    fp16=True,\n",
    "    max_grad_norm=0.3,\n",
    "    num_train_epochs=4,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=0.2,\n",
    "    warmup_ratio=0.05,\n",
    "    save_strategy=\"epoch\",\n",
    "    group_by_length=True,\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    report_to=\"tensorboard\",\n",
    "    save_safetensors=True,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    seed=42,\n",
    ")\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-27T16:41:54.993122Z",
     "iopub.status.busy": "2023-12-27T16:41:54.992721Z",
     "iopub.status.idle": "2023-12-27T20:26:51.344620Z",
     "shell.execute_reply": "2023-12-27T20:26:51.343660Z",
     "shell.execute_reply.started": "2023-12-27T16:41:54.993080Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a99191e32d04f18a67eecb373ca8149",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d93367aca9794aad93f14b04c4e3f0e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='248' max='248' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [248/248 3:43:52, Epoch 3/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.749700</td>\n",
       "      <td>1.725374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.772800</td>\n",
       "      <td>1.718492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.644800</td>\n",
       "      <td>1.719420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.534000</td>\n",
       "      <td>1.719500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=248, training_loss=1.6479617496651988, metrics={'train_runtime': 13484.3523, 'train_samples_per_second': 0.297, 'train_steps_per_second': 0.018, 'total_flos': 7.123436335212134e+16, 'train_loss': 1.6479617496651988, 'epoch': 3.97})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=validation_data,\n",
    "    peft_config=lora_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=1024,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-27T23:37:26.108415Z",
     "iopub.status.busy": "2023-12-27T23:37:26.107982Z",
     "iopub.status.idle": "2023-12-27T23:37:26.209048Z",
     "shell.execute_reply": "2023-12-27T23:37:26.208124Z",
     "shell.execute_reply.started": "2023-12-27T23:37:26.108383Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./peft-tokenizer/tokenizer_config.json',\n",
       " './peft-tokenizer/special_tokens_map.json',\n",
       " './peft-tokenizer/tokenizer.model',\n",
       " './peft-tokenizer/added_tokens.json',\n",
       " './peft-tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_model_path=\"./peft-dialogue-summary\"\n",
    "\n",
    "trainer.model.save_pretrained(peft_model_path)\n",
    "tokenizer.save_pretrained(peft_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **INFERENCE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-27T20:41:33.416328Z",
     "iopub.status.busy": "2023-12-27T20:41:33.415958Z",
     "iopub.status.idle": "2023-12-27T20:41:33.448446Z",
     "shell.execute_reply": "2023-12-27T20:41:33.447619Z",
     "shell.execute_reply.started": "2023-12-27T20:41:33.416298Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TextStreamer\n",
    "model.config.use_cache = True\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-27T20:41:34.634173Z",
     "iopub.status.busy": "2023-12-27T20:41:34.633517Z",
     "iopub.status.idle": "2023-12-27T20:41:34.638735Z",
     "shell.execute_reply": "2023-12-27T20:41:34.637577Z",
     "shell.execute_reply.started": "2023-12-27T20:41:34.634140Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"TOKEN\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-27T23:12:06.135094Z",
     "iopub.status.busy": "2023-12-27T23:12:06.134045Z",
     "iopub.status.idle": "2023-12-27T23:12:17.455014Z",
     "shell.execute_reply": "2023-12-27T23:12:17.453911Z",
     "shell.execute_reply.started": "2023-12-27T23:12:06.135045Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cdd0d21382147798a28d0a5a3a81d7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "peft_model_dir = \"peft-dialogue-summary\"\n",
    "\n",
    "# load base LLM model and tokenizer\n",
    "trained_model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    peft_model_dir,\n",
    "    low_cpu_mem_usage=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(peft_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-27T23:32:19.787030Z",
     "iopub.status.busy": "2023-12-27T23:32:19.786030Z",
     "iopub.status.idle": "2023-12-27T23:32:39.833407Z",
     "shell.execute_reply": "2023-12-27T23:32:39.832415Z",
     "shell.execute_reply.started": "2023-12-27T23:32:19.786990Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "\n",
      "Summarize the following conversation.\n",
      "\n",
      "### Input:\n",
      "By . Daily Mail Reporter . PUBLISHED: . 05:21 EST, 11 June 2013 . | . UPDATED: . 07:11 EST, 11 June 2013 . The reckless owners of a white water rafting firm have been jailed following a campaign by the parents of a  British schoolgirl who drowned after being thrown out of an overcrowded boat. Tragic Cerys Potter, nine, was on holiday with her family in south-west Turkey when the raft capsized as it was going down rapids. A court heard Cerys was the ninth holidaymaker to drown in rafting accidents on the Dalaman river.Marmaris Rafting described their river trips as a 'family experience' suitable for children. Justice: Cerys Potter, who died in a rafting accident, with her mother Julie and brother James September 2008.  The Turkish owners of the rafting company were today jailed for recklessness leading to her death . The firm even sold souvenir £40 DVDs showing dramatic footage of rafters being thrown out of boats as they capsized on rocks. On July 28, 2010, Cerys, of Llancarfan, near Cardiff, . was riding in one of their inflatable boats with her brother, aunt, . uncle and cousins when she tragically died. The boat should have carried a maximum of eight passengers but had 12 holidaymakers on board. Cerys and her 12-year-old cousin became trapped between the raft and a rock. Her cousin was rescued from the river unconscious and spent three days in intensive care. Cerys's body was found washed-up on rocks more than an hour later. The schoolgirl's parents Terry and . Julie Potter campaigned for justice for their daughter and for . safeguards to prevent further tragedies. They were dismayed to discover that the day after Cerys's death Maramis rafting appeared to bve operating as normal. And when Mr Potter  returned to the Dalaman river to gather evidence more than a week . after Marmaris Rafting had lost its licence, he found it operating . illegally. He and his wife were in court as the . panel of four judges found two owners and two employees of Marmaris . Rafting guilty of causing death by recklessness. They were each sentenced to five and half years in prison by the court in Mugla, South West, Turkey. Terry and Julie Potter fought a tireless campaign to bring those responsible for their daughter's death to justice . Mr Potter . said afterwards: 'We couldn't sit back and know that there were still . children going on these death traps - and more children were going to . get killed. 'Nothing can bring back Cerys, but I . want people to realise the risks that are being taken on that river, and . do all I can to prevent another death.' 'This should put a clear message that . they can't carry on killing British holidaymakers including children . and just get away with it. 'It is sad that it has taken the . deaths of nine holidaymakers on this rafting trip to get to this stage . but we do hope that they get the message now.' The men, all from Turkey, have 10 days to appeal against their convictions. For the official investigation at the High Court in Mugla, Terence and . Julie have orchestrated witness statements and even paid for one witness . to fly to Turkey to present her statement. In an interview before the beginning of the trial Mr Potter told how his daughter had been thrown from the raft. Four adults, four children and a . guide were thrown out of the front of the raft and carried downstream, . but the four children at the back – Cerys, James, Marcus and a friend . called James – were washed to a small gap between the rock and the steep . bank. 'The water was extremely deep. Cerys was dragged down by the swell of the water and drowned,' says Terence. Terry Potter (centre white t-shirt) whose daughter Cerys died in a Turkish rafting accident, goes back to Turkey to experience white water rafting on the same stretch of river where his daughter died . Tourists and guides pulled out the . three boys. When Marcus was found unconscious, an ambulance was called . and in the pandemonium that followed, few noticed that Cerys was . missing. Terence says: 'James shouted to everyone, 'Where's Cerys?' But one of . the guides said she had been taken to safety in another raft.' Julie adds: 'I've only seen James cry once. The night after the accident, he sobbed all night, saying, “I couldn't save her.” ' Only when the party, plus 53 other tourists who had been on other rafts . along the Dalaman, were taken to base camp, was it clear that Cerys was . missing. 'The alarm was raised loudest by  a holidaymaker in another of the . rafts,' Julie says. It was only then – an hour after the raft capsized – . that the guides returned. With the river  levels lowered, they found . Cerys. Vale of Glamorgan MP Alun Cairns said the Potter family had been instrumental in ensuring justice for their daughter. He said: 'I pay tribute to Terry and . July Potter who have been relentless in their pursuit of this process in . the interests of other holidaymakers. 'The family have channelled their . energy from such a terrible tragedy to ensure no-one else experiences . the same kind of incident. 'The scale of the sentence sends a . strong message that tourist operators cannot get away with their blatant . disregard of health and safety standards.'\n",
      "\n",
      "### Summary:\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "Cerys Potter, nine, was on holiday with her family in south-west Turkey .\n",
      "Her raft capsized on the Dalaman river as it was going down rapids .\n",
      "She was the ninth holidaymaker to drown in rafting accidents on the river .\n",
      "The firm described the trips as a 'family experience' suitable for children .\n",
      "They were open for business and operating normally the following day .\n",
      "Two owners and two workers found guilty of causing death by recklessness .\n",
      "They were each sentenced to five and half years in prison by the court .\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "TRAINED MODEL GENERATED TEXT :\n",
      "Cerys Potter, nine, was on holiday with her family in south-west Turkey when the raft capsized as it was going down rapids .\n",
      "The court heard Cerys was the ninth holidaymaker to drown in rafting accidents on the Dalaman river .\n",
      "The Turkish owners of the rafting company were jailed for recklessness leading to her death .\n",
      "The Potter family have campaigned for justice for their daughter and for safeguards to prevent further tragedies .\n",
      "Terry and Julie Potter orchestrated witness statements and even paid for one witness to fly to Turkey to present her statement .\n",
      "The Potters have channelled their energy from\n"
     ]
    }
   ],
   "source": [
    "index = 51\n",
    "\n",
    "dialogue = train_data['article'][index][:10000]\n",
    "summary = train_data['highlights'][index]\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "### Input:\n",
    "{dialogue}\n",
    "\n",
    "### Summary:\n",
    "\"\"\"\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors='pt',truncation=True).input_ids.cuda()\n",
    "outputs = trained_model.generate(input_ids=input_ids, max_new_tokens=200, )\n",
    "output= tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0][len(prompt):]\n",
    "\n",
    "dash_line = '-'.join('' for x in range(100))\n",
    "print(dash_line)\n",
    "print(f'INPUT PROMPT:\\n{prompt}')\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n",
    "print(dash_line)\n",
    "print(f'TRAINED MODEL GENERATED TEXT :\\n{output}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 4221454,
     "sourceId": 7280730,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30627,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
